{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 3: Reconocimiento de imágenes\n",
    "## Aplicación en detección de Retinopatía Diabética\n",
    "### Introducción\n",
    "La retinopatía diabética es una enfermedad ocular que puede producirse en pacientes con diabetes. Se da debido a los altos niveles de azúcar en la sangre que causan daño a los vasos sanguíneos en la retina. Es general que no se presenten síntomas en las etapas tempranas. Los síntomas de la retinopatía diabética suelen afectar a ambos ojos. A medida que empeora la enfermedad, se presentan los siguientes síntomas:\n",
    "\n",
    "* Un mayor número de moscas volantes\n",
    "* Visión borrosa\n",
    "* Visión que cambia de borrosa a clara\n",
    "* Ver áreas en blanco u oscuras en el campo de visión\n",
    "* Visión nocturna deficiente\n",
    "* Notar que los colores se ven atenuados o apagados\n",
    "* Pérdida la visión\n",
    "\n",
    "#### Etapas y detección\n",
    "La retinopatía se puede detectar de múltiples maneras, sin embargo existe un método a través del cual este padecimiento es detectado por inspección visual de los ojos del paciente. Los ojos, específicamente “un diferencial del ojo” se pueden observar nubosidades, que son hemorragias de los vasos sanguíneos.\n",
    "![DR Stages](./assets/etapas.png)\n",
    "![RD Detección](./assets/RetinopatiaDiabetica.jpg)\n",
    "\n",
    "##### Tipos de retinopatía diabética\n",
    "* Retinopatía diabética proliferativa: Esta es la etapa más avanzada de la enfermedad. Se produce cuando la retina comienza a desarrollar nuevos vasos sanguíneos. Esto se denomina neovascularización. Estos vasos nuevos frágiles a menudo sangran hacia el vítreo. La retinopatía diabética proliferativa es muy grave y puede hacerle perder tanto la visión central como la periférica.\n",
    "\n",
    "* Retinopatía diabética no proliferativa: Esta es la etapa temprana de la enfermedad ocular diabética. La mayoría de las personas diabéticas padecen de este tipo de retinopatía. En esta etapa de la enfermedad, muchos vasos sanguíneos pequeños sufren pérdidas y hacen que la retina se hinche. Si  se tiene retinopatía diabética no proliferativa, la visión del paciente será borrosa\n",
    "\n",
    "##### Problemas de detección\n",
    "![Tensorflow Diagnosis](./assets/tensorflow.png)\n",
    "La gráfica anterior muestra que incluso oftalmólogos pueden presentar inconsistencias a la hora de diagnosticar a un paciente como con, o sin, retinopatía diabética y el grado de la misma.\n",
    "\n",
    "### Análisis del dataset\n",
    "\n",
    "Las imágenes son de varias dimensiones, habiendo de 1050 x 1050 px, 3216 x 2136 px, entre otros, a color por lo que se recomienda modificar las imágenes a escala de grises, o bien una transformación que permita reducir la complejidad de la red neuronal, logrando reducir el uso de recursos computacionales. \n",
    "\n",
    "El dataset provisto contiene 3,662 observaciones para entrenar y 1,928 para testear. Las columnas que contiene el dataset es de código - de la cual no se presentará un análisis pues no brinda información esencial - y diagnosis, esta variable se distribuye de la siguiente manera:\n",
    "\n",
    "![Original Train Diagnosis Histogram](./assets/diagnosisHist.png)\n",
    "![Original Train Diagnosis Proportions](./assets/props.png)\n",
    "\n",
    "El código para realizar las visualizaciones anteriores puede encontrarse en el archivo DataExploration.R\n",
    "\n",
    "Se puede observar que la mayoría de imágenes, siendo casi un 50%, en el dataset de entrenamiento son diagnosticadas como “sin retinopatía diabética”, mientras que menos de un 5.3% y 8.1% son clasificadas como “severa” y “proliferativa”, respectivamente; esto puede repercutir en las respuestas que se obtengan con la red neuronal al introducir el test set, ya que podría sesgar la información y clasificar como “sin diagnosis”.\n",
    "\n",
    "Algunas de las imágenes para cada diagnóstico son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Para importar las imagenes\n",
    "from PIL import Image # Para visualizar las imagenes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./csvs/train.csv')\n",
    "\n",
    "# for i in range(611):\n",
    "#     img = Image.open('./train_images/' + train_df['id_code'][i] + '.png').convert('LA')\n",
    "#     new_image = img.resize((256, 256))\n",
    "#     new_image.save('./transform_images/' + train_df['id_code'][i] + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0 - No DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9b37c945b03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mno_dr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnosis\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/8/DATA SCIENCE/l3/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "no_dr = Image.open(train_df[train_df.diagnosis==0].iloc[0].path)\n",
    "plt.imshow(np.asarray(no_dr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Mild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mild = Image.open(train_df[train_df.diagnosis==1].iloc[0].path)\n",
    "plt.imshow(np.asarray(mild))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Moderate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate = Image.open(train_df[train_df.diagnosis==2].iloc[0].path)\n",
    "plt.imshow(np.asarray(moderate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 - Severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sev = Image.open(train_df[train_df.diagnosis==3].iloc[0].path)\n",
    "plt.imshow(np.asarray(sev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Proliferative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prol = Image.open(train_df[train_df.diagnosis==4].iloc[0].path)\n",
    "plt.imshow(np.asarray(prol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['path'] = train_df['id_code'].map(lambda x: os.path.join('./train_images/','{}.png'.format(x)))\n",
    "train_df['path_gray'] = train_df['id_code'].map(lambda x: os.path.join('./transform_images/','{}.png'.format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[:610] #se usaran solo las primeras 611 fotos, por cuestiones de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_labs = pd.read_csv('./csvs/diagnosis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_labs = pd.merge(train_df, diagnosis_labs)\n",
    "train_df_labs[\"diagnosis_n\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que, ya que los diagnósticos estan distribuidos aleatoriamente en el csv original, escoger los primeros 611 no parece afectar a la distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No lo vamos a usar todavia, pero importemoslo de una vez\n",
    "submission = pd.read_csv('./csvs/test.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modificación de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver dimensiones de imagen\n",
    "im = Image.open(train_df['path'][1])\n",
    "width, height = im.size\n",
    "print(width,height) \n",
    "plt.imshow(np.asarray(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que las imagenes son bastante grandes, por lo cual será necesario para empezar, cambiar su tamaño.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, creemos nuestros train y test set a partir de train_df. Escogeremos 65% y 35% al azar, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = np.split(train_df.sample(frac=1), [int(.65*len(train_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('train_set.npy', train)\n",
    "# np.save('test_set.npy', test)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train=pd.DataFrame(np.load('train_set.npy', allow_pickle=True), columns=['id_code', 'diagnosis','path', 'path_gray'])\n",
    "test=pd.DataFrame(np.load('test_set.npy', allow_pickle=True), columns=['id_code', 'diagnosis','path', 'path_gray'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se debe procurar compensar la sobre-representación de los casos sin retinopatía diabética en el dataset de train, ya que esto podría ser un bias que afectaría el cross validation y el test.\n",
    "\n",
    "Para esto, se hará un oversampling.\n",
    "Para referencia, esta es la distribución de nuestro nuevo train set, al escoger una muestra del 65% al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labs = pd.merge(train, diagnosis_labs)\n",
    "train_labs[\"diagnosis_n\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que la distribución de los diagnósticos se sigue manteniendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = train_labs['diagnosis_n'].value_counts().max()\n",
    "max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_size = train_labs['diagnosis_n'].value_counts().min()\n",
    "min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el oversampling\n",
    "# lst = [train_labs]\n",
    "# for class_index, group in train_labs.groupby('diagnosis'):\n",
    "#    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "# oversampled_train = pd.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('oversampled_train.npy',oversampled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oversampled_train=pd.DataFrame(np.load('oversampled_train.npy', allow_pickle=True), columns=['id_code',\n",
    "                                                                                             'diagnosis',\n",
    "                                                                                             'path',\n",
    "                                                                                             'path_gray',\n",
    "                                                                                             'diagnosis_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "oversampled_train[\"diagnosis_n\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_dr = np.random.choice(train_labs[train_labs.diagnosis==0].index, min_size, replace=False)\n",
    "# mild = np.random.choice(train_labs[train_labs.diagnosis==1].index, min_size, replace=False)\n",
    "# moderate = np.random.choice(train_labs[train_labs.diagnosis==2].index, min_size, replace=False)\n",
    "# severe = np.random.choice(train_labs[train_labs.diagnosis==3].index, min_size, replace=False)\n",
    "# proliferative = np.random.choice(train_labs[train_labs.diagnosis==4].index, min_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample_indexes = np.concatenate([no_dr,mild,moderate,severe,proliferative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# undersampled_train = train_labs.loc[undersample_indexes]\n",
    "# np.save('undersampled_train.npy',undersampled_train)\n",
    "undersampled_train=pd.DataFrame(np.load('undersampled_train.npy', allow_pickle=True), columns=['id_code',\n",
    "                                                                                             'diagnosis',\n",
    "                                                                                             'path',\n",
    "                                                                                             'path_gray',\n",
    "                                                                                             'diagnosis_n'])\n",
    "undersampled_train[\"diagnosis_n\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones y Redes Neuronales\n",
    "Se realizarán 2 modelos, uno con el dataset undersampled y el otro oversampled. Para empezar, cargaremos las librerías a utilizar y los datos del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 361.84it/s]\n"
     ]
    }
   ],
   "source": [
    "test_images = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    img = image.load_img(test[\"path_gray\"][i], target_size=(256,256, 1), color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_images.append(img)\n",
    "X_test = np.array(test_images)\n",
    "\n",
    "y_test = test['diagnosis'].values\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>path</th>\n",
       "      <th>path_gray</th>\n",
       "      <th>diagnosis_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0cbcc7b23613</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/0cbcc7b23613.png</td>\n",
       "      <td>./transform_images/0cbcc7b23613.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0232dfea7547</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/0232dfea7547.png</td>\n",
       "      <td>./transform_images/0232dfea7547.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ae3c58759fb</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/1ae3c58759fb.png</td>\n",
       "      <td>./transform_images/1ae3c58759fb.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240b25a7debe</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/240b25a7debe.png</td>\n",
       "      <td>./transform_images/240b25a7debe.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17f6c7072f61</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/17f6c7072f61.png</td>\n",
       "      <td>./transform_images/17f6c7072f61.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>165cd2070ebd</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/165cd2070ebd.png</td>\n",
       "      <td>./transform_images/165cd2070ebd.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1e9224ccca95</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/1e9224ccca95.png</td>\n",
       "      <td>./transform_images/1e9224ccca95.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0243404e8a00</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/0243404e8a00.png</td>\n",
       "      <td>./transform_images/0243404e8a00.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>262ad704319c</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/262ad704319c.png</td>\n",
       "      <td>./transform_images/262ad704319c.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0e82bcacc475</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/0e82bcacc475.png</td>\n",
       "      <td>./transform_images/0e82bcacc475.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id_code diagnosis                             path  \\\n",
       "0   0cbcc7b23613         0  ./train_images/0cbcc7b23613.png   \n",
       "1   0232dfea7547         0  ./train_images/0232dfea7547.png   \n",
       "2   1ae3c58759fb         0  ./train_images/1ae3c58759fb.png   \n",
       "3   240b25a7debe         0  ./train_images/240b25a7debe.png   \n",
       "4   17f6c7072f61         0  ./train_images/17f6c7072f61.png   \n",
       "..           ...       ...                              ...   \n",
       "90  165cd2070ebd         4  ./train_images/165cd2070ebd.png   \n",
       "91  1e9224ccca95         4  ./train_images/1e9224ccca95.png   \n",
       "92  0243404e8a00         4  ./train_images/0243404e8a00.png   \n",
       "93  262ad704319c         4  ./train_images/262ad704319c.png   \n",
       "94  0e82bcacc475         4  ./train_images/0e82bcacc475.png   \n",
       "\n",
       "                              path_gray    diagnosis_n  \n",
       "0   ./transform_images/0cbcc7b23613.png          No DR  \n",
       "1   ./transform_images/0232dfea7547.png          No DR  \n",
       "2   ./transform_images/1ae3c58759fb.png          No DR  \n",
       "3   ./transform_images/240b25a7debe.png          No DR  \n",
       "4   ./transform_images/17f6c7072f61.png          No DR  \n",
       "..                                  ...            ...  \n",
       "90  ./transform_images/165cd2070ebd.png  Proliferative  \n",
       "91  ./transform_images/1e9224ccca95.png  Proliferative  \n",
       "92  ./transform_images/0243404e8a00.png  Proliferative  \n",
       "93  ./transform_images/262ad704319c.png  Proliferative  \n",
       "94  ./transform_images/0e82bcacc475.png  Proliferative  \n",
       "\n",
       "[95 rows x 5 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersampled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que son tan pocas entradas, se estima que la predicción de este modelo sea más baja que la del oversampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:00<00:00, 309.61it/s]\n"
     ]
    }
   ],
   "source": [
    "undersampled_images = []\n",
    "for i in tqdm(range(undersampled_train.shape[0])):\n",
    "    img = image.load_img(undersampled_train[\"path_gray\"][i], target_size=(256,256,1), color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    undersampled_images.append(img)\n",
    "X_undersampled = np.array(undersampled_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_undersampled = undersampled_train['diagnosis'].values\n",
    "y_undersampled = to_categorical(y_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>path</th>\n",
       "      <th>path_gray</th>\n",
       "      <th>diagnosis_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10bf25731c08</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/10bf25731c08.png</td>\n",
       "      <td>./transform_images/10bf25731c08.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28503940d10b</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/28503940d10b.png</td>\n",
       "      <td>./transform_images/28503940d10b.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1ec95179cdfe</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/1ec95179cdfe.png</td>\n",
       "      <td>./transform_images/1ec95179cdfe.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0e43c8298fc0</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/0e43c8298fc0.png</td>\n",
       "      <td>./transform_images/0e43c8298fc0.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a38b552372d</td>\n",
       "      <td>0</td>\n",
       "      <td>./train_images/0a38b552372d.png</td>\n",
       "      <td>./transform_images/0a38b552372d.png</td>\n",
       "      <td>No DR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>20d5fdd450ae</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/20d5fdd450ae.png</td>\n",
       "      <td>./transform_images/20d5fdd450ae.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0ada12c0e78f</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/0ada12c0e78f.png</td>\n",
       "      <td>./transform_images/0ada12c0e78f.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>0318598cfd16</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/0318598cfd16.png</td>\n",
       "      <td>./transform_images/0318598cfd16.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>2017cd92c63d</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/2017cd92c63d.png</td>\n",
       "      <td>./transform_images/2017cd92c63d.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>1e9224ccca95</td>\n",
       "      <td>4</td>\n",
       "      <td>./train_images/1e9224ccca95.png</td>\n",
       "      <td>./transform_images/1e9224ccca95.png</td>\n",
       "      <td>Proliferative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>915 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_code diagnosis                             path  \\\n",
       "0    10bf25731c08         0  ./train_images/10bf25731c08.png   \n",
       "1    28503940d10b         0  ./train_images/28503940d10b.png   \n",
       "2    1ec95179cdfe         0  ./train_images/1ec95179cdfe.png   \n",
       "3    0e43c8298fc0         0  ./train_images/0e43c8298fc0.png   \n",
       "4    0a38b552372d         0  ./train_images/0a38b552372d.png   \n",
       "..            ...       ...                              ...   \n",
       "910  20d5fdd450ae         4  ./train_images/20d5fdd450ae.png   \n",
       "911  0ada12c0e78f         4  ./train_images/0ada12c0e78f.png   \n",
       "912  0318598cfd16         4  ./train_images/0318598cfd16.png   \n",
       "913  2017cd92c63d         4  ./train_images/2017cd92c63d.png   \n",
       "914  1e9224ccca95         4  ./train_images/1e9224ccca95.png   \n",
       "\n",
       "                               path_gray    diagnosis_n  \n",
       "0    ./transform_images/10bf25731c08.png          No DR  \n",
       "1    ./transform_images/28503940d10b.png          No DR  \n",
       "2    ./transform_images/1ec95179cdfe.png          No DR  \n",
       "3    ./transform_images/0e43c8298fc0.png          No DR  \n",
       "4    ./transform_images/0a38b552372d.png          No DR  \n",
       "..                                   ...            ...  \n",
       "910  ./transform_images/20d5fdd450ae.png  Proliferative  \n",
       "911  ./transform_images/0ada12c0e78f.png  Proliferative  \n",
       "912  ./transform_images/0318598cfd16.png  Proliferative  \n",
       "913  ./transform_images/2017cd92c63d.png  Proliferative  \n",
       "914  ./transform_images/1e9224ccca95.png  Proliferative  \n",
       "\n",
       "[915 rows x 5 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 915/915 [00:02<00:00, 347.54it/s]\n"
     ]
    }
   ],
   "source": [
    "oversampled_images = []\n",
    "for i in tqdm(range(oversampled_train.shape[0])):\n",
    "    img = image.load_img(oversampled_train[\"path_gray\"][i], target_size=(256,256,1), color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    oversampled_images.append(img)\n",
    "X_oversampled = np.array(oversampled_images)\n",
    "\n",
    "y_oversampled = oversampled_train['diagnosis'].values\n",
    "y_oversampled = to_categorical(y_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del modelo 1 -- Este fue el mejor con 10 épocas.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(256,256,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate = 0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 24.7842 - accuracy: 0.1474 - val_loss: 7.5064 - val_accuracy: 0.4439\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 14.2278 - accuracy: 0.2211 - val_loss: 5.8985 - val_accuracy: 0.1028\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 3.1042 - accuracy: 0.3474 - val_loss: 1.1290 - val_accuracy: 0.5374\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 14s 5s/step - loss: 1.6016 - accuracy: 0.3158 - val_loss: 1.0617 - val_accuracy: 0.5935\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.2170 - accuracy: 0.4632 - val_loss: 1.1794 - val_accuracy: 0.4439\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.1377 - accuracy: 0.5368 - val_loss: 1.0899 - val_accuracy: 0.5374\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0543 - accuracy: 0.5789 - val_loss: 1.1037 - val_accuracy: 0.5514\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 0.9641 - accuracy: 0.6105 - val_loss: 1.2848 - val_accuracy: 0.4299\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.9312 - accuracy: 0.6211 - val_loss: 1.2407 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.9427 - accuracy: 0.6211 - val_loss: 1.1503 - val_accuracy: 0.5654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15ccfb520>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_undersampled, y_undersampled, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampled Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del modelo 1 -- Este fue el mejor con 10 épocas.\n",
    "model_over = Sequential()\n",
    "model_over.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(256,256,1)))\n",
    "model_over.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_over.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_over.add(Dropout(rate = 0.25))\n",
    "model_over.add(Flatten())\n",
    "model_over.add(Dense(128, activation='relu'))\n",
    "model_over.add(Dropout(rate = 0.5))\n",
    "model_over.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_over.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 104s 4s/step - loss: 3.0349 - accuracy: 0.4448 - val_loss: 1.0750 - val_accuracy: 0.5093\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 105s 4s/step - loss: 0.8670 - accuracy: 0.6962 - val_loss: 1.1512 - val_accuracy: 0.5654\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 106s 4s/step - loss: 0.6287 - accuracy: 0.7792 - val_loss: 1.1831 - val_accuracy: 0.5561\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 107s 4s/step - loss: 0.4081 - accuracy: 0.8634 - val_loss: 1.2588 - val_accuracy: 0.6402\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 105s 4s/step - loss: 0.2338 - accuracy: 0.9235 - val_loss: 1.2745 - val_accuracy: 0.6075\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 112s 4s/step - loss: 0.1643 - accuracy: 0.9475 - val_loss: 1.1833 - val_accuracy: 0.6542\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 106s 4s/step - loss: 0.1471 - accuracy: 0.9541 - val_loss: 1.2175 - val_accuracy: 0.6168\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 105s 4s/step - loss: 0.0896 - accuracy: 0.9760 - val_loss: 1.3290 - val_accuracy: 0.6495\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 105s 4s/step - loss: 0.0575 - accuracy: 0.9858 - val_loss: 1.5145 - val_accuracy: 0.6449\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 106s 4s/step - loss: 0.0608 - accuracy: 0.9836 - val_loss: 1.4225 - val_accuracy: 0.6355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169026460>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_over.fit(X_oversampled, y_oversampled, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
